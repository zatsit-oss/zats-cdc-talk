# zats-cdc-talk
Project to illustrate the CDC talk

## Getting Started

This project includes a full-stack application with a Kafka Connect stack, a frontend, and a backend. You can use the provided `Makefile` to manage and start the services easily.

### Prerequisites

Ensure you have the following installed on your system:
- Docker and Docker Compose
- Node.js and npm

### Available Commands

The `Makefile` provides the following commands:

- **Start Kafka Connect stack:**
  ```bash
  make connect
  ```
  This command starts the Kafka Connect stack using the `kafka/full-stack.yml` file.

- **Start Ngrok:**
  ```bash
  make ngrok
  ```
  This command starts Ngrok service to proxy frontend, backend and kafka REST proxy. You need to modify [ngrok.yml](./ngrok.yml) before, to put your Ngrok Auth Token.

- **Get URLs generated by Ngrok**
  ```bash
  make get-ngrok-urls
  ```
  This command create a new file `.env.ngrok.generated` and query Ngrok services to get URLs generated for frontend, backend and Kafka REST Proxy. Also, this command put these URL into environment variables:
  - Frontend side:
    - VITE_SOCKET_URL: backend URL for Websocket communication
    - VITE_KAFKA_PROXY_URL: Kafka REST Proxy for Kafka Producer
  - Backend side:
    - FRONTEND_URL

- **Start Frontend:**
  ```bash
  make frontend
  ```
  This command starts the frontend development server. You need to install deps with `npm i` before run this

- **Start Backend:**
  ```bash
  make backend
  ```
  This command starts the backend development server. You need to install deps with `npm i` before run this

- **Start All Services:**
  ```bash
  make start
  ```
  This command starts the Kafka Connect stack, ngrok, backend and frontend with ngrok exposed urls.

- **Stop All Services:**
  ```bash
  make stop
  ```
  This command stops all running containers (connect).


### How to configure Kafka Connect and create a Postgresql connector ?

Kafka Connect is already available with the provided docker-compose file. You just need to run make start.

#### Database configuration

At startup, our Postgresql database is already with a wal level at `logical` which is the level required to work with Kafka Connect.
You can easily verify with the following command: Ì€`show wal_level;`

After this, you need to run some queries to make the heartbeat working:

```sql
CREATE TABLE IF NOT EXISTS public.kafka_connect_heartbeat
(
    name text COLLATE pg_catalog."default",
    count bigint,
    CONSTRAINT kafka_connect_heartbeat_pkey PRIMARY KEY (name)

);

INSERT INTO public.kafka_connect_heartbeat values ('kafka_connect_heartbeat',1);
```

#### How to add a Postgresql connector


You have two choices to achieve this.

1. With Conduktor Console

- Go to the Conduktor Console, by default http://localhost:8080
- Go to "Kafka Connect" on left menu
- Click on "full stack kafka connect"
- Click on "Add connector" on the top right corner
- Choose "Postgres Connector"
- On the top right corner, choose "raw" mode and paste the following configuration:

```json
{
  "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
  "database.hostname": "postgresql-pokesky",
  "database.port": "5432",
  "database.user": "pikachu",
  "database.password": "zatsit",
  "database.dbname": "pokesky",
  "database.server.name": "pokesky",
  "plugin.name": "pgoutput",
  "topic.prefix": "pokesky",
  "topic.heartbeat.prefix": "pokesky",
  "heartbeat.interval.ms": 10000,
  "heartbeat.action.query": "update public.kafka_connect_heartbeat set count=count+1 where name='kafka_connect_heartbeat'",
  "topic.creation.default.partitions": 1,
  "topic.creation.default.replication.factor": 1,
  "snapshot.mode": "never",
  "value.converter.schemas.enable": "false",
  "key.converter.schemas.enable": "false",
  "key.converter": "org.apache.kafka.connect.json.JsonConverter",
  "value.converter": "org.apache.kafka.connect.json.JsonConverter"
}
```
- Give a name for your connector (on the top) and validate the configuration
- Click on Next
- Review your config and click on Submit
- And ðŸŽ‰

2. REST API

Kafka Connect also provides an API to manage connectors. You can find reference [here](https://docs.confluent.io/platform/current/connect/references/restapi.html)

To create a connector with API, you can:

Make a POST request to this endpoint: `http://localhost:8083/connectors`
And fill the body with the following content:
```json
{
  "name": "pokesky-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "postgresql-pokesky",
    "database.port": "5432",
    "database.user": "pikachu",
    "database.password": "zatsit",
    "database.dbname": "pokesky",
    "database.server.name": "pokesky",
    "plugin.name": "pgoutput",
    "topic.prefix": "pokesky",
    "topic.heartbeat.prefix": "pokesky",
    "heartbeat.interval.ms": 10000,
    "heartbeat.action.query": "update public.kafka_connect_heartbeat set count=count+1 where name='kafka_connect_heartbeat'",
    "topic.creation.default.partitions": 1,
    "topic.creation.default.replication.factor": 1,
    "snapshot.mode": "never",
    "value.converter.schemas.enable": "false",
    "key.converter.schemas.enable": "false",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter"
  }
}
```

To ensure your connector is running, you can hit `http://localhost:8083/connectors/:name/status` (replace `:name` by your connector's name)

And ðŸŽ‰

### Notes

- Ensure that the `docker-compose` command is available in your terminal.
- The frontend and backend directories are located under the `package/` folder.
- Modify the `Makefile` if your directory structure or requirements differ.
- To kill ngrok, frontend and backend services started with make start command, just Ctrl + C

